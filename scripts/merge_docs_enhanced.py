#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¢å¼ºç‰ˆæ–‡æ¡£ä¸»é¢˜åˆå¹¶è„šæœ¬
ç”¨äºè·¨ç›®å½•è‡ªåŠ¨è¯†åˆ«å¹¶åˆå¹¶docsç›®å½•ä¸‹ç›¸åŒä¸»é¢˜çš„æ–‡æ¡£
"""

import os
import re
import json
import shutil
from datetime import datetime
from pathlib import Path
from collections import defaultdict
from typing import Dict, List, Tuple, Set
import logging

class EnhancedDocumentMerger:
    def __init__(self, docs_path: str = "./docs"):
        self.docs_path = Path(docs_path)
        self.backup_path = Path("./docs_backup_enhanced")
        self.log_path = Path("./logs")
        self.setup_logging()
        
        # ä¸»é¢˜å…³é”®è¯æ˜ å°„ï¼ˆå¢å¼ºç‰ˆï¼‰
        self.theme_keywords = {
            "äº¤æ˜“è®°å½•": ["äº¤æ˜“è®°å½•", "transaction", "è®°å½•é¡µé¢", "æ—¥æœŸç­›é€‰", "æŸ¥è¯¢åŠŸèƒ½", "äº¤æ˜“", "è®°å½•"],
            "èµ„äº§ç®¡ç†": ["èµ„äº§", "assets", "èµ„äº§é¡µé¢", "èµ„äº§æ•°æ®", "å†å²å¿«ç…§", "èµ„äº§ç®¡ç†"],
            "é¢„ç®—ç®¡ç†": ["é¢„ç®—", "budget", "é¢„ç®—å¯¹æ¯”", "é¢„ç®—æ•°æ®", "é¢„ç®—ç®¡ç†"],
            "å®¶åº­åä½œ": ["å®¶åº­åä½œ", "å®¶åº­ç®¡ç†", "åä½œåŠŸèƒ½", "æˆå‘˜ç®¡ç†", "å®¶åº­", "åä½œ"],
            "æŠ¥è¡¨ç»Ÿè®¡": ["æŠ¥è¡¨", "ç»Ÿè®¡", "reports", "è¶‹åŠ¿å›¾", "åˆ†æ", "æŠ¥è¡¨ç»Ÿè®¡"],
            "é—®é¢˜ä¿®å¤": ["ä¿®å¤", "fix", "é—®é¢˜", "é”™è¯¯", "bug", "é—®é¢˜ä¿®å¤"],
            "åŠŸèƒ½ä¼˜åŒ–": ["ä¼˜åŒ–", "optimization", "æ”¹è¿›", "enhancement", "åŠŸèƒ½ä¼˜åŒ–"],
            "æµ‹è¯•ç›¸å…³": ["æµ‹è¯•", "test", "éªŒè¯", "æ£€æŸ¥", "æµ‹è¯•ç›¸å…³"],
            "æ•°æ®åº“ç›¸å…³": ["æ•°æ®åº“", "database", "é›†åˆ", "ç´¢å¼•", "äº‘å¼€å‘", "æ•°æ®åº“ç›¸å…³"],
            "UIç•Œé¢": ["UI", "ç•Œé¢", "äº¤äº’", "å¯¼èˆª", "å®‰å…¨åŒº", "UIç•Œé¢"],
            "æŠ€æœ¯æ–‡æ¡£": ["æŠ€æœ¯", "API", "æ¥å£", "æ¶æ„", "è®¾è®¡", "æŠ€æœ¯æ–‡æ¡£"],
            "å…¼å®¹æ€§": ["å…¼å®¹", "compatibility", "é€‚é…", "ç‰ˆæœ¬", "å…¼å®¹æ€§"]
        }
        
        # æ–‡æ¡£é‡è¦æ€§æƒé‡
        self.importance_weights = {
            "åˆå¹¶æ–‡æ¡£": 15,  # å·²åˆå¹¶çš„æ–‡æ¡£ä¼˜å…ˆçº§æœ€é«˜
            "æ€»ç»“": 10,
            "å®Œæˆ": 9,
            "æœ€ç»ˆ": 8,
            "æ–¹æ¡ˆ": 7,
            "æŠ¥å‘Š": 6,
            "è®°å½•": 5,
            "åˆ†æ": 4,
            "è®¡åˆ’": 3,
            "æŒ‡å—": 2
        }

    def setup_logging(self):
        """è®¾ç½®æ—¥å¿—ç³»ç»Ÿ"""
        self.log_path.mkdir(exist_ok=True)
        log_file = self.log_path / f"doc_merge_enhanced_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log"
        
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(log_file, encoding='utf-8'),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger(__name__)

    def create_backup(self):
        """åˆ›å»ºæ–‡æ¡£å¤‡ä»½"""
        try:
            if self.backup_path.exists():
                shutil.rmtree(self.backup_path)
            shutil.copytree(self.docs_path, self.backup_path)
            self.logger.info(f"âœ… å·²åˆ›å»ºå¤‡ä»½: {self.backup_path}")
        except Exception as e:
            self.logger.error(f"âŒ å¤‡ä»½åˆ›å»ºå¤±è´¥: {e}")
            raise

    def scan_documents(self) -> List[Path]:
        """æ‰«ææ‰€æœ‰æ–‡æ¡£æ–‡ä»¶ï¼ˆè·¨ç›®å½•ï¼‰"""
        documents = []
        supported_extensions = {'.md', '.txt', '.docx', '.doc'}
        
        # è·¨ç›®å½•é€’å½’æœç´¢
        for root, dirs, files in os.walk(self.docs_path):
            # è·³è¿‡å¤‡ä»½å’Œä¸´æ—¶ç›®å½•
            dirs[:] = [d for d in dirs if not d.startswith('.') and d not in ['_archived', '_backup']]
            
            for file in files:
                file_path = Path(root) / file
                if file_path.suffix.lower() in supported_extensions:
                    # è·³è¿‡å·²ç»æ˜¯åˆå¹¶æ–‡æ¡£çš„æ–‡ä»¶ï¼ˆé¿å…é‡å¤åˆå¹¶ï¼‰
                    if not file.endswith('_åˆå¹¶æ–‡æ¡£.md'):
                        documents.append(file_path)
        
        self.logger.info(f"ğŸ“„ è·¨ç›®å½•æ‰«æåˆ° {len(documents)} ä¸ªæ–‡æ¡£æ–‡ä»¶")
        return documents

    def classify_by_theme(self, documents: List[Path]) -> Dict[str, List[Path]]:
        """æŒ‰ä¸»é¢˜åˆ†ç±»æ–‡æ¡£ï¼ˆå¢å¼ºç‰ˆï¼‰"""
        theme_groups = defaultdict(list)
        unclassified = []
        
        for doc_path in documents:
            doc_name = doc_path.stem
            doc_content = self.read_document_content(doc_path)
            relative_path = str(doc_path.relative_to(self.docs_path))
            
            # æŸ¥æ‰¾åŒ¹é…çš„ä¸»é¢˜
            matched_themes = []
            for theme, keywords in self.theme_keywords.items():
                score = 0
                
                # æ£€æŸ¥æ–‡ä»¶å
                for keyword in keywords:
                    if keyword in doc_name.lower():
                        score += 3  # æ–‡ä»¶ååŒ¹é…æƒé‡æ›´é«˜
                
                # æ£€æŸ¥æ–‡ä»¶è·¯å¾„
                for keyword in keywords:
                    if keyword in relative_path.lower():
                        score += 2  # è·¯å¾„åŒ¹é…
                
                # æ£€æŸ¥æ–‡æ¡£å†…å®¹
                for keyword in keywords:
                    if keyword in doc_content[:2000].lower():  # æ£€æŸ¥å‰2000å­—ç¬¦
                        score += 1
                
                if score > 0:
                    matched_themes.append((theme, score))
            
            if matched_themes:
                # é€‰æ‹©å¾—åˆ†æœ€é«˜çš„ä¸»é¢˜
                best_theme = max(matched_themes, key=lambda x: x[1])[0]
                theme_groups[best_theme].append(doc_path)
                self.logger.debug(f"ğŸ“‹ {doc_path.name} â†’ {best_theme} (å¾—åˆ†: {max(matched_themes, key=lambda x: x[1])[1]})")
            else:
                unclassified.append(doc_path)
        
        # è®°å½•åˆ†ç±»ç»“æœ
        for theme, docs in theme_groups.items():
            self.logger.info(f"ğŸ“‚ {theme}: {len(docs)} ä¸ªæ–‡æ¡£")
            for doc in docs:
                self.logger.debug(f"   - {doc.relative_to(self.docs_path)}")
        
        if unclassified:
            self.logger.info(f"â“ æœªåˆ†ç±»: {len(unclassified)} ä¸ªæ–‡æ¡£")
            theme_groups["æœªåˆ†ç±»"] = unclassified
        
        return dict(theme_groups)

    def read_document_content(self, doc_path: Path) -> str:
        """è¯»å–æ–‡æ¡£å†…å®¹"""
        try:
            if doc_path.suffix.lower() in ['.md', '.txt']:
                with open(doc_path, 'r', encoding='utf-8') as f:
                    return f.read()
            elif doc_path.suffix.lower() in ['.docx', '.doc']:
                # å¯¹äºWordæ–‡æ¡£ï¼Œè¿™é‡Œç®€åŒ–å¤„ç†
                return doc_path.stem
            else:
                return ""
        except Exception as e:
            self.logger.warning(f"âš ï¸ æ— æ³•è¯»å–æ–‡æ¡£ {doc_path}: {e}")
            return ""

    def calculate_document_importance(self, doc_path: Path, content: str) -> int:
        """è®¡ç®—æ–‡æ¡£é‡è¦æ€§å¾—åˆ†"""
        score = 0
        doc_name = doc_path.stem.lower()
        
        # åŸºäºæ–‡ä»¶åå…³é”®è¯è¯„åˆ†
        for keyword, weight in self.importance_weights.items():
            if keyword in doc_name:
                score += weight
        
        # åŸºäºå†…å®¹é•¿åº¦è¯„åˆ†
        content_length = len(content)
        if content_length > 10000:
            score += 8
        elif content_length > 5000:
            score += 5
        elif content_length > 2000:
            score += 3
        elif content_length > 500:
            score += 1
        
        # åŸºäºç»“æ„å®Œæ•´æ€§è¯„åˆ†
        if '# ' in content:  # æœ‰æ ‡é¢˜
            score += 3
        if '## ' in content:  # æœ‰äºŒçº§æ ‡é¢˜
            score += 2
        if '```' in content:  # æœ‰ä»£ç å—
            score += 1
        if '---' in content:  # æœ‰åˆ†éš”ç¬¦
            score += 1
        
        # åŸºäºæ–‡ä»¶ä½ç½®è¯„åˆ†
        if 'analysis-reports' in str(doc_path):
            score += 2  # åˆ†ææŠ¥å‘Šç›®å½•çš„æ–‡æ¡£ä¼˜å…ˆçº§è¾ƒé«˜
        
        return score

    def select_master_document(self, docs: List[Path]) -> Path:
        """é€‰æ‹©ä¸»æ–‡æ¡£"""
        if len(docs) == 1:
            return docs[0]
        
        scored_docs = []
        for doc in docs:
            content = self.read_document_content(doc)
            score = self.calculate_document_importance(doc, content)
            scored_docs.append((doc, score, len(content)))
        
        # æŒ‰å¾—åˆ†æ’åºï¼Œå¾—åˆ†ç›¸åŒæ—¶æŒ‰å†…å®¹é•¿åº¦æ’åº
        scored_docs.sort(key=lambda x: (x[1], x[2]), reverse=True)
        
        master_doc = scored_docs[0][0]
        self.logger.info(f"ğŸ“‹ é€‰æ‹©ä¸»æ–‡æ¡£: {master_doc.name} (å¾—åˆ†: {scored_docs[0][1]})")
        
        return master_doc

    def extract_key_information(self, docs: List[Path], master_doc: Path) -> str:
        """æå–å¹¶åˆå¹¶å…³é”®ä¿¡æ¯"""
        merged_content = []
        
        # è¯»å–ä¸»æ–‡æ¡£å†…å®¹
        master_content = self.read_document_content(master_doc)
        
        # æ·»åŠ åˆå¹¶è¯´æ˜å¤´éƒ¨
        header = f"""# {self.get_clean_title(master_doc.stem)}

> ğŸ“ æœ¬æ–‡æ¡£ç”± {len(docs)} ä¸ªç›¸å…³æ–‡æ¡£åˆå¹¶è€Œæˆ
> ğŸ•’ åˆå¹¶æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
> ğŸ“‚ åŸå§‹æ–‡æ¡£æ•°é‡: {len(docs)}
> ğŸ¯ ä¸»æ–‡æ¡£: {master_doc.name}

---

"""
        merged_content.append(header)
        
        # æ·»åŠ ä¸»æ–‡æ¡£å†…å®¹
        if master_content:
            merged_content.append("## ä¸»è¦å†…å®¹\n\n")
            # æ¸…ç†ä¸»æ–‡æ¡£å†…å®¹ä¸­çš„é‡å¤æ ‡é¢˜
            cleaned_content = self.clean_content(master_content)
            merged_content.append(cleaned_content)
            merged_content.append("\n\n---\n\n")
        
        # å¤„ç†å…¶ä»–æ–‡æ¡£
        other_docs = [doc for doc in docs if doc != master_doc]
        if other_docs:
            merged_content.append("## è¡¥å……ä¿¡æ¯\n\n")
            
            for i, doc in enumerate(other_docs, 1):
                content = self.read_document_content(doc)
                if content and len(content.strip()) > 100:  # åªå¤„ç†æœ‰å®è´¨å†…å®¹çš„æ–‡æ¡£
                    merged_content.append(f"### {i}. æ¥æº: {doc.name}\n\n")
                    
                    # æå–å…³é”®æ®µè½
                    key_sections = self.extract_key_sections(content)
                    if key_sections.strip():
                        merged_content.append(key_sections)
                        merged_content.append("\n\n")
        
        # æ·»åŠ åŸå§‹æ–‡æ¡£åˆ—è¡¨
        merged_content.append("## åŸå§‹æ–‡æ¡£åˆ—è¡¨\n\n")
        for i, doc in enumerate(docs, 1):
            relative_path = doc.relative_to(self.docs_path)
            merged_content.append(f"{i}. `{relative_path}`\n")
        
        merged_content.append(f"\n---\n\n*åˆå¹¶å®Œæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}*\n")
        
        return "".join(merged_content)

    def get_clean_title(self, filename: str) -> str:
        """è·å–æ¸…ç†åçš„æ ‡é¢˜"""
        # ç§»é™¤å¸¸è§çš„åç¼€
        title = filename.replace('_åˆå¹¶æ–‡æ¡£', '').replace('-åˆå¹¶', '').replace('_æ€»ç»“', '')
        return title

    def clean_content(self, content: str) -> str:
        """æ¸…ç†æ–‡æ¡£å†…å®¹"""
        lines = content.split('\n')
        cleaned_lines = []
        
        for line in lines:
            # è·³è¿‡é‡å¤çš„åˆå¹¶è¯´æ˜
            if 'æœ¬æ–‡æ¡£ç”±' in line and 'åˆå¹¶è€Œæˆ' in line:
                continue
            if 'åˆå¹¶æ—¶é—´:' in line:
                continue
            if 'åŸå§‹æ–‡æ¡£æ•°é‡:' in line:
                continue
            
            cleaned_lines.append(line)
        
        return '\n'.join(cleaned_lines)

    def extract_key_sections(self, content: str) -> str:
        """æå–æ–‡æ¡£çš„å…³é”®æ®µè½"""
        lines = content.split('\n')
        key_lines = []
        in_code_block = False
        
        for line in lines:
            line_stripped = line.strip()
            
            # å¤„ç†ä»£ç å—
            if line_stripped.startswith('```'):
                in_code_block = not in_code_block
                key_lines.append(line)
                continue
            
            if in_code_block:
                key_lines.append(line)
                continue
            
            if not line_stripped:
                if key_lines and key_lines[-1].strip():  # é¿å…è¿ç»­ç©ºè¡Œ
                    key_lines.append('')
                continue
            
            # ä¿ç•™æ ‡é¢˜
            if line_stripped.startswith('#'):
                key_lines.append(line)
            # ä¿ç•™é‡è¦ä¿¡æ¯è¡Œ
            elif any(keyword in line_stripped for keyword in ['é—®é¢˜', 'è§£å†³', 'ä¿®å¤', 'ä¼˜åŒ–', 'ç»“æœ', 'æ€»ç»“', 'å®Œæˆ', 'å®ç°']):
                key_lines.append(line)
            # ä¿ç•™åˆ—è¡¨é¡¹
            elif line_stripped.startswith(('-', '*', '+')):
                key_lines.append(line)
            # ä¿ç•™æ•°å­—åˆ—è¡¨
            elif re.match(r'^\d+\.', line_stripped):
                key_lines.append(line)
        
        # é™åˆ¶é•¿åº¦å¹¶æ¸…ç†
        if len(key_lines) > 50:
            key_lines = key_lines[:50]
            key_lines.append("...")
        
        # ç§»é™¤æœ«å°¾çš„ç©ºè¡Œ
        while key_lines and not key_lines[-1].strip():
            key_lines.pop()
        
        return '\n'.join(key_lines)

    def merge_theme_documents(self, theme: str, docs: List[Path]) -> bool:
        """åˆå¹¶åŒä¸»é¢˜æ–‡æ¡£"""
        try:
            if len(docs) <= 1:
                self.logger.info(f"â­ï¸ {theme}: åªæœ‰ {len(docs)} ä¸ªæ–‡æ¡£ï¼Œè·³è¿‡åˆå¹¶")
                return True
            
            self.logger.info(f"ğŸ”„ å¼€å§‹åˆå¹¶ {theme}: {len(docs)} ä¸ªæ–‡æ¡£")
            
            # é€‰æ‹©ä¸»æ–‡æ¡£
            master_doc = self.select_master_document(docs)
            
            # æå–å¹¶åˆå¹¶å…³é”®ä¿¡æ¯
            merged_content = self.extract_key_information(docs, master_doc)
            
            # åˆ›å»ºåˆå¹¶åçš„æ–‡æ¡£åå’Œè·¯å¾„
            safe_theme = re.sub(r'[^\w\-_]', '_', theme)
            merged_filename = f"{safe_theme}_åˆå¹¶æ–‡æ¡£.md"
            
            # å°†åˆå¹¶æ–‡æ¡£æ”¾åœ¨docsæ ¹ç›®å½•
            merged_path = self.docs_path / merged_filename
            
            # å†™å…¥åˆå¹¶åçš„æ–‡æ¡£
            with open(merged_path, 'w', encoding='utf-8') as f:
                f.write(merged_content)
            
            # åˆ é™¤åŸå§‹æ–‡æ¡£
            deleted_count = 0
            for doc in docs:
                if doc != merged_path:  # é¿å…åˆ é™¤åˆšåˆ›å»ºçš„åˆå¹¶æ–‡æ¡£
                    try:
                        doc.unlink()
                        deleted_count += 1
                        self.logger.info(f"ğŸ—‘ï¸ å·²åˆ é™¤: {doc.relative_to(self.docs_path)}")
                    except Exception as e:
                        self.logger.warning(f"âš ï¸ åˆ é™¤å¤±è´¥ {doc.name}: {e}")
            
            self.logger.info(f"âœ… {theme} åˆå¹¶å®Œæˆ: {merged_filename} (åˆ é™¤äº† {deleted_count} ä¸ªåŸæ–‡æ¡£)")
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ {theme} åˆå¹¶å¤±è´¥: {e}")
            return False

    def generate_summary_report(self, results: Dict[str, bool]):
        """ç”Ÿæˆåˆå¹¶æ€»ç»“æŠ¥å‘Š"""
        report_content = f"""# å¢å¼ºç‰ˆæ–‡æ¡£åˆå¹¶æ€»ç»“æŠ¥å‘Š

## æ‰§è¡Œä¿¡æ¯
- æ‰§è¡Œæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
- å¤„ç†ç›®å½•: {self.docs_path}
- å¤‡ä»½ä½ç½®: {self.backup_path}
- åˆå¹¶ç­–ç•¥: è·¨ç›®å½•æ™ºèƒ½ä¸»é¢˜è¯†åˆ«

## åˆå¹¶ç»“æœç»Ÿè®¡
"""
        
        successful = sum(1 for success in results.values() if success)
        total = len(results)
        
        report_content += f"- æ€»ä¸»é¢˜æ•°: {total}\n"
        report_content += f"- æˆåŠŸåˆå¹¶: {successful}\n"
        report_content += f"- å¤±è´¥æ•°é‡: {total - successful}\n"
        report_content += f"- æˆåŠŸç‡: {successful/total*100:.1f}%\n\n"
        
        report_content += "## è¯¦ç»†åˆå¹¶ç»“æœ\n\n"
        for theme, success in results.items():
            status = "âœ… æˆåŠŸ" if success else "âŒ å¤±è´¥"
            report_content += f"- **{theme}**: {status}\n"
        
        report_content += f"\n## åˆå¹¶åæ–‡æ¡£ä½ç½®\n\n"
        report_content += "æ‰€æœ‰åˆå¹¶åçš„æ–‡æ¡£éƒ½ä½äº `docs/` æ ¹ç›®å½•ä¸‹ï¼Œæ–‡ä»¶åæ ¼å¼ä¸º `{ä¸»é¢˜}_åˆå¹¶æ–‡æ¡£.md`\n\n"
        
        report_content += f"## å¤‡ä»½ä¿¡æ¯\n\n"
        report_content += f"- åŸå§‹æ–‡æ¡£å·²å¤‡ä»½è‡³: `{self.backup_path}`\n"
        report_content += f"- å¦‚éœ€æ¢å¤ï¼Œè¯·æ‰‹åŠ¨å¤åˆ¶å¤‡ä»½æ–‡ä»¶\n\n"
        
        report_content += f"## æ—¥å¿—æ–‡ä»¶\n\n"
        report_content += f"è¯¦ç»†æ“ä½œæ—¥å¿—è¯·æŸ¥çœ‹: `{self.log_path}/doc_merge_enhanced_*.log`\n"
        
        # ä¿å­˜æŠ¥å‘Š
        report_path = self.docs_path / "å¢å¼ºç‰ˆæ–‡æ¡£åˆå¹¶æ€»ç»“æŠ¥å‘Š.md"
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write(report_content)
        
        self.logger.info(f"ğŸ“Š æ€»ç»“æŠ¥å‘Šå·²ç”Ÿæˆ: {report_path}")

    def run(self):
        """æ‰§è¡Œæ–‡æ¡£åˆå¹¶æµç¨‹"""
        try:
            self.logger.info("ğŸš€ å¼€å§‹å¢å¼ºç‰ˆæ–‡æ¡£åˆå¹¶æµç¨‹")
            
            # åˆ›å»ºå¤‡ä»½
            self.create_backup()
            
            # æ‰«ææ–‡æ¡£
            documents = self.scan_documents()
            if not documents:
                self.logger.warning("âš ï¸ æœªæ‰¾åˆ°ä»»ä½•æ–‡æ¡£æ–‡ä»¶")
                return
            
            # æŒ‰ä¸»é¢˜åˆ†ç±»
            theme_groups = self.classify_by_theme(documents)
            
            # åˆå¹¶æ¯ä¸ªä¸»é¢˜çš„æ–‡æ¡£
            results = {}
            for theme, docs in theme_groups.items():
                results[theme] = self.merge_theme_documents(theme, docs)
            
            # ç”Ÿæˆæ€»ç»“æŠ¥å‘Š
            self.generate_summary_report(results)
            
            self.logger.info("ğŸ‰ å¢å¼ºç‰ˆæ–‡æ¡£åˆå¹¶æµç¨‹å®Œæˆ")
            
        except Exception as e:
            self.logger.error(f"ğŸ’¥ æ–‡æ¡£åˆå¹¶æµç¨‹å¤±è´¥: {e}")
            raise

def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description='å¢å¼ºç‰ˆæ–‡æ¡£ä¸»é¢˜åˆå¹¶å·¥å…·')
    parser.add_argument('--docs-path', default='./docs', help='æ–‡æ¡£ç›®å½•è·¯å¾„')
    parser.add_argument('--dry-run', action='store_true', help='è¯•è¿è¡Œæ¨¡å¼ï¼ˆä¸å®é™…ä¿®æ”¹æ–‡ä»¶ï¼‰')
    
    args = parser.parse_args()
    
    if args.dry_run:
        print("ğŸ” è¯•è¿è¡Œæ¨¡å¼ - ä¸ä¼šå®é™…ä¿®æ”¹æ–‡ä»¶")
    
    merger = EnhancedDocumentMerger(args.docs_path)
    merger.run()

if __name__ == "__main__":
    main()