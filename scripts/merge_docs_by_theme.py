#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ–‡æ¡£ä¸»é¢˜åˆå¹¶è„šæœ¬
ç”¨äºè‡ªåŠ¨è¯†åˆ«å¹¶åˆå¹¶docsç›®å½•ä¸‹ç›¸åŒä¸»é¢˜çš„æ–‡æ¡£
"""

import os
import re
import json
import shutil
from datetime import datetime
from pathlib import Path
from collections import defaultdict
from typing import Dict, List, Tuple, Set
import logging

class DocumentMerger:
    def __init__(self, docs_path: str = "./docs"):
        self.docs_path = Path(docs_path)
        self.backup_path = Path("./docs_backup")
        self.log_path = Path("./logs")
        self.setup_logging()
        
        # ä¸»é¢˜å…³é”®è¯æ˜ å°„
        self.theme_keywords = {
            "äº¤æ˜“è®°å½•": ["äº¤æ˜“è®°å½•", "transaction", "è®°å½•é¡µé¢", "æ—¥æœŸç­›é€‰", "æŸ¥è¯¢åŠŸèƒ½"],
            "èµ„äº§ç®¡ç†": ["èµ„äº§", "assets", "èµ„äº§é¡µé¢", "èµ„äº§æ•°æ®", "å†å²å¿«ç…§"],
            "é¢„ç®—ç®¡ç†": ["é¢„ç®—", "budget", "é¢„ç®—å¯¹æ¯”", "é¢„ç®—æ•°æ®"],
            "å®¶åº­åä½œ": ["å®¶åº­åä½œ", "å®¶åº­ç®¡ç†", "åä½œåŠŸèƒ½", "æˆå‘˜ç®¡ç†"],
            "æŠ¥è¡¨ç»Ÿè®¡": ["æŠ¥è¡¨", "ç»Ÿè®¡", "reports", "è¶‹åŠ¿å›¾", "åˆ†æ"],
            "é—®é¢˜ä¿®å¤": ["ä¿®å¤", "fix", "é—®é¢˜", "é”™è¯¯", "bug"],
            "åŠŸèƒ½ä¼˜åŒ–": ["ä¼˜åŒ–", "optimization", "æ”¹è¿›", "enhancement"],
            "æµ‹è¯•ç›¸å…³": ["æµ‹è¯•", "test", "éªŒè¯", "æ£€æŸ¥"],
            "æ•°æ®åº“ç›¸å…³": ["æ•°æ®åº“", "database", "é›†åˆ", "ç´¢å¼•", "äº‘å¼€å‘"],
            "UIç•Œé¢": ["UI", "ç•Œé¢", "äº¤äº’", "å¯¼èˆª", "å®‰å…¨åŒº"],
            "æŠ€æœ¯æ–‡æ¡£": ["æŠ€æœ¯", "API", "æ¥å£", "æ¶æ„", "è®¾è®¡"],
            "å…¼å®¹æ€§": ["å…¼å®¹", "compatibility", "é€‚é…", "ç‰ˆæœ¬"]
        }
        
        # æ–‡æ¡£é‡è¦æ€§æƒé‡
        self.importance_weights = {
            "æ€»ç»“": 10,
            "å®Œæˆ": 9,
            "æœ€ç»ˆ": 8,
            "æ–¹æ¡ˆ": 7,
            "æŠ¥å‘Š": 6,
            "è®°å½•": 5,
            "åˆ†æ": 4,
            "è®¡åˆ’": 3,
            "æŒ‡å—": 2
        }

    def setup_logging(self):
        """è®¾ç½®æ—¥å¿—ç³»ç»Ÿ"""
        self.log_path.mkdir(exist_ok=True)
        log_file = self.log_path / f"doc_merge_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log"
        
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(log_file, encoding='utf-8'),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger(__name__)

    def create_backup(self):
        """åˆ›å»ºæ–‡æ¡£å¤‡ä»½"""
        try:
            if self.backup_path.exists():
                shutil.rmtree(self.backup_path)
            shutil.copytree(self.docs_path, self.backup_path)
            self.logger.info(f"âœ… å·²åˆ›å»ºå¤‡ä»½: {self.backup_path}")
        except Exception as e:
            self.logger.error(f"âŒ å¤‡ä»½åˆ›å»ºå¤±è´¥: {e}")
            raise

    def scan_documents(self) -> List[Path]:
        """æ‰«ææ‰€æœ‰æ–‡æ¡£æ–‡ä»¶"""
        documents = []
        supported_extensions = {'.md', '.txt', '.docx', '.doc'}
        
        for root, dirs, files in os.walk(self.docs_path):
            # è·³è¿‡å¤‡ä»½å’Œä¸´æ—¶ç›®å½•
            dirs[:] = [d for d in dirs if not d.startswith('.') and d != '_archived']
            
            for file in files:
                file_path = Path(root) / file
                if file_path.suffix.lower() in supported_extensions:
                    documents.append(file_path)
        
        self.logger.info(f"ğŸ“„ æ‰«æåˆ° {len(documents)} ä¸ªæ–‡æ¡£æ–‡ä»¶")
        return documents

    def classify_by_theme(self, documents: List[Path]) -> Dict[str, List[Path]]:
        """æŒ‰ä¸»é¢˜åˆ†ç±»æ–‡æ¡£"""
        theme_groups = defaultdict(list)
        unclassified = []
        
        for doc_path in documents:
            doc_name = doc_path.stem
            doc_content = self.read_document_content(doc_path)
            
            # æŸ¥æ‰¾åŒ¹é…çš„ä¸»é¢˜
            matched_themes = []
            for theme, keywords in self.theme_keywords.items():
                score = 0
                for keyword in keywords:
                    if keyword in doc_name or keyword in doc_content[:1000]:  # åªæ£€æŸ¥å‰1000å­—ç¬¦
                        score += 1
                
                if score > 0:
                    matched_themes.append((theme, score))
            
            if matched_themes:
                # é€‰æ‹©å¾—åˆ†æœ€é«˜çš„ä¸»é¢˜
                best_theme = max(matched_themes, key=lambda x: x[1])[0]
                theme_groups[best_theme].append(doc_path)
            else:
                unclassified.append(doc_path)
        
        # è®°å½•åˆ†ç±»ç»“æœ
        for theme, docs in theme_groups.items():
            self.logger.info(f"ğŸ“‚ {theme}: {len(docs)} ä¸ªæ–‡æ¡£")
        
        if unclassified:
            self.logger.info(f"â“ æœªåˆ†ç±»: {len(unclassified)} ä¸ªæ–‡æ¡£")
            theme_groups["æœªåˆ†ç±»"] = unclassified
        
        return dict(theme_groups)

    def read_document_content(self, doc_path: Path) -> str:
        """è¯»å–æ–‡æ¡£å†…å®¹"""
        try:
            if doc_path.suffix.lower() == '.md':
                with open(doc_path, 'r', encoding='utf-8') as f:
                    return f.read()
            elif doc_path.suffix.lower() == '.txt':
                with open(doc_path, 'r', encoding='utf-8') as f:
                    return f.read()
            elif doc_path.suffix.lower() in ['.docx', '.doc']:
                # å¯¹äºWordæ–‡æ¡£ï¼Œè¿™é‡Œç®€åŒ–å¤„ç†ï¼Œå®é™…å¯ä»¥ä½¿ç”¨python-docxåº“
                return doc_path.stem  # æš‚æ—¶åªè¿”å›æ–‡ä»¶å
            else:
                return ""
        except Exception as e:
            self.logger.warning(f"âš ï¸ æ— æ³•è¯»å–æ–‡æ¡£ {doc_path}: {e}")
            return ""

    def calculate_document_importance(self, doc_path: Path, content: str) -> int:
        """è®¡ç®—æ–‡æ¡£é‡è¦æ€§å¾—åˆ†"""
        score = 0
        doc_name = doc_path.stem
        
        # åŸºäºæ–‡ä»¶åå…³é”®è¯è¯„åˆ†
        for keyword, weight in self.importance_weights.items():
            if keyword in doc_name:
                score += weight
        
        # åŸºäºå†…å®¹é•¿åº¦è¯„åˆ†
        content_length = len(content)
        if content_length > 5000:
            score += 5
        elif content_length > 2000:
            score += 3
        elif content_length > 500:
            score += 1
        
        # åŸºäºç»“æ„å®Œæ•´æ€§è¯„åˆ†
        if '# ' in content:  # æœ‰æ ‡é¢˜
            score += 2
        if '## ' in content:  # æœ‰äºŒçº§æ ‡é¢˜
            score += 1
        if '```' in content:  # æœ‰ä»£ç å—
            score += 1
        
        return score

    def select_master_document(self, docs: List[Path]) -> Path:
        """é€‰æ‹©ä¸»æ–‡æ¡£"""
        if len(docs) == 1:
            return docs[0]
        
        scored_docs = []
        for doc in docs:
            content = self.read_document_content(doc)
            score = self.calculate_document_importance(doc, content)
            scored_docs.append((doc, score, len(content)))
        
        # æŒ‰å¾—åˆ†æ’åºï¼Œå¾—åˆ†ç›¸åŒæ—¶æŒ‰å†…å®¹é•¿åº¦æ’åº
        scored_docs.sort(key=lambda x: (x[1], x[2]), reverse=True)
        
        master_doc = scored_docs[0][0]
        self.logger.info(f"ğŸ“‹ é€‰æ‹©ä¸»æ–‡æ¡£: {master_doc.name} (å¾—åˆ†: {scored_docs[0][1]})")
        
        return master_doc

    def extract_key_information(self, docs: List[Path], master_doc: Path) -> str:
        """æå–å¹¶åˆå¹¶å…³é”®ä¿¡æ¯"""
        merged_content = []
        
        # è¯»å–ä¸»æ–‡æ¡£å†…å®¹
        master_content = self.read_document_content(master_doc)
        
        # æ·»åŠ åˆå¹¶è¯´æ˜å¤´éƒ¨
        header = f"""# {master_doc.stem}

> ğŸ“ æœ¬æ–‡æ¡£ç”±å¤šä¸ªç›¸å…³æ–‡æ¡£åˆå¹¶è€Œæˆ
> ğŸ•’ åˆå¹¶æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
> ğŸ“‚ åŸå§‹æ–‡æ¡£æ•°é‡: {len(docs)}

---

"""
        merged_content.append(header)
        
        # æ·»åŠ ä¸»æ–‡æ¡£å†…å®¹
        if master_content:
            merged_content.append("## ä¸»è¦å†…å®¹\n")
            merged_content.append(master_content)
            merged_content.append("\n---\n")
        
        # å¤„ç†å…¶ä»–æ–‡æ¡£
        other_docs = [doc for doc in docs if doc != master_doc]
        if other_docs:
            merged_content.append("## è¡¥å……ä¿¡æ¯\n")
            
            for doc in other_docs:
                content = self.read_document_content(doc)
                if content and len(content.strip()) > 50:  # åªå¤„ç†æœ‰å®è´¨å†…å®¹çš„æ–‡æ¡£
                    merged_content.append(f"### æ¥æº: {doc.name}\n")
                    
                    # æå–å…³é”®æ®µè½ï¼ˆç®€åŒ–å¤„ç†ï¼‰
                    key_sections = self.extract_key_sections(content)
                    merged_content.append(key_sections)
                    merged_content.append("\n")
        
        # æ·»åŠ åŸå§‹æ–‡æ¡£åˆ—è¡¨
        merged_content.append("## åŸå§‹æ–‡æ¡£åˆ—è¡¨\n")
        for i, doc in enumerate(docs, 1):
            relative_path = doc.relative_to(self.docs_path)
            merged_content.append(f"{i}. `{relative_path}`\n")
        
        return "".join(merged_content)

    def extract_key_sections(self, content: str) -> str:
        """æå–æ–‡æ¡£çš„å…³é”®æ®µè½"""
        lines = content.split('\n')
        key_lines = []
        
        for line in lines:
            line = line.strip()
            if not line:
                continue
                
            # ä¿ç•™æ ‡é¢˜
            if line.startswith('#'):
                key_lines.append(line)
            # ä¿ç•™é‡è¦ä¿¡æ¯è¡Œ
            elif any(keyword in line for keyword in ['é—®é¢˜', 'è§£å†³', 'ä¿®å¤', 'ä¼˜åŒ–', 'ç»“æœ', 'æ€»ç»“']):
                key_lines.append(line)
            # ä¿ç•™ä»£ç å—
            elif line.startswith('```'):
                key_lines.append(line)
        
        # é™åˆ¶é•¿åº¦
        if len(key_lines) > 20:
            key_lines = key_lines[:20]
            key_lines.append("...")
        
        return '\n'.join(key_lines) + '\n'

    def merge_theme_documents(self, theme: str, docs: List[Path]) -> bool:
        """åˆå¹¶åŒä¸»é¢˜æ–‡æ¡£"""
        try:
            if len(docs) <= 1:
                self.logger.info(f"â­ï¸ {theme}: åªæœ‰ {len(docs)} ä¸ªæ–‡æ¡£ï¼Œè·³è¿‡åˆå¹¶")
                return True
            
            self.logger.info(f"ğŸ”„ å¼€å§‹åˆå¹¶ {theme}: {len(docs)} ä¸ªæ–‡æ¡£")
            
            # é€‰æ‹©ä¸»æ–‡æ¡£
            master_doc = self.select_master_document(docs)
            
            # æå–å¹¶åˆå¹¶å…³é”®ä¿¡æ¯
            merged_content = self.extract_key_information(docs, master_doc)
            
            # åˆ›å»ºåˆå¹¶åçš„æ–‡æ¡£å
            safe_theme = re.sub(r'[^\w\-_]', '_', theme)
            merged_filename = f"{safe_theme}_åˆå¹¶æ–‡æ¡£.md"
            merged_path = master_doc.parent / merged_filename
            
            # å†™å…¥åˆå¹¶åçš„æ–‡æ¡£
            with open(merged_path, 'w', encoding='utf-8') as f:
                f.write(merged_content)
            
            # åˆ é™¤åŸå§‹æ–‡æ¡£ï¼ˆé™¤äº†ä¸»æ–‡æ¡£ï¼Œå¦‚æœä¸»æ–‡æ¡£åä¸åŒçš„è¯ï¼‰
            deleted_count = 0
            for doc in docs:
                if doc != merged_path:  # é¿å…åˆ é™¤åˆšåˆ›å»ºçš„åˆå¹¶æ–‡æ¡£
                    try:
                        doc.unlink()
                        deleted_count += 1
                        self.logger.info(f"ğŸ—‘ï¸ å·²åˆ é™¤: {doc.name}")
                    except Exception as e:
                        self.logger.warning(f"âš ï¸ åˆ é™¤å¤±è´¥ {doc.name}: {e}")
            
            self.logger.info(f"âœ… {theme} åˆå¹¶å®Œæˆ: {merged_path.name} (åˆ é™¤äº† {deleted_count} ä¸ªåŸæ–‡æ¡£)")
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ {theme} åˆå¹¶å¤±è´¥: {e}")
            return False

    def generate_summary_report(self, results: Dict[str, bool]):
        """ç”Ÿæˆåˆå¹¶æ€»ç»“æŠ¥å‘Š"""
        report_content = f"""# æ–‡æ¡£åˆå¹¶æ€»ç»“æŠ¥å‘Š

## æ‰§è¡Œä¿¡æ¯
- æ‰§è¡Œæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
- å¤„ç†ç›®å½•: {self.docs_path}
- å¤‡ä»½ä½ç½®: {self.backup_path}

## åˆå¹¶ç»“æœ
"""
        
        successful = sum(1 for success in results.values() if success)
        total = len(results)
        
        report_content += f"- æ€»ä¸»é¢˜æ•°: {total}\n"
        report_content += f"- æˆåŠŸåˆå¹¶: {successful}\n"
        report_content += f"- å¤±è´¥æ•°é‡: {total - successful}\n\n"
        
        report_content += "## è¯¦ç»†ç»“æœ\n"
        for theme, success in results.items():
            status = "âœ… æˆåŠŸ" if success else "âŒ å¤±è´¥"
            report_content += f"- {theme}: {status}\n"
        
        report_content += f"\n## æ—¥å¿—æ–‡ä»¶\n"
        report_content += f"è¯¦ç»†æ—¥å¿—è¯·æŸ¥çœ‹: `{self.log_path}/doc_merge_*.log`\n"
        
        # ä¿å­˜æŠ¥å‘Š
        report_path = self.docs_path / "æ–‡æ¡£åˆå¹¶æ€»ç»“æŠ¥å‘Š.md"
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write(report_content)
        
        self.logger.info(f"ğŸ“Š æ€»ç»“æŠ¥å‘Šå·²ç”Ÿæˆ: {report_path}")

    def run(self):
        """æ‰§è¡Œæ–‡æ¡£åˆå¹¶æµç¨‹"""
        try:
            self.logger.info("ğŸš€ å¼€å§‹æ–‡æ¡£åˆå¹¶æµç¨‹")
            
            # åˆ›å»ºå¤‡ä»½
            self.create_backup()
            
            # æ‰«ææ–‡æ¡£
            documents = self.scan_documents()
            if not documents:
                self.logger.warning("âš ï¸ æœªæ‰¾åˆ°ä»»ä½•æ–‡æ¡£æ–‡ä»¶")
                return
            
            # æŒ‰ä¸»é¢˜åˆ†ç±»
            theme_groups = self.classify_by_theme(documents)
            
            # åˆå¹¶æ¯ä¸ªä¸»é¢˜çš„æ–‡æ¡£
            results = {}
            for theme, docs in theme_groups.items():
                if len(docs) > 1:  # åªåˆå¹¶æœ‰å¤šä¸ªæ–‡æ¡£çš„ä¸»é¢˜
                    results[theme] = self.merge_theme_documents(theme, docs)
                else:
                    self.logger.info(f"â­ï¸ {theme}: åªæœ‰1ä¸ªæ–‡æ¡£ï¼Œè·³è¿‡åˆå¹¶")
                    results[theme] = True
            
            # ç”Ÿæˆæ€»ç»“æŠ¥å‘Š
            self.generate_summary_report(results)
            
            self.logger.info("ğŸ‰ æ–‡æ¡£åˆå¹¶æµç¨‹å®Œæˆ")
            
        except Exception as e:
            self.logger.error(f"ğŸ’¥ æ–‡æ¡£åˆå¹¶æµç¨‹å¤±è´¥: {e}")
            raise

def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description='æ–‡æ¡£ä¸»é¢˜åˆå¹¶å·¥å…·')
    parser.add_argument('--docs-path', default='./docs', help='æ–‡æ¡£ç›®å½•è·¯å¾„')
    parser.add_argument('--dry-run', action='store_true', help='è¯•è¿è¡Œæ¨¡å¼ï¼ˆä¸å®é™…ä¿®æ”¹æ–‡ä»¶ï¼‰')
    
    args = parser.parse_args()
    
    if args.dry_run:
        print("ğŸ” è¯•è¿è¡Œæ¨¡å¼ - ä¸ä¼šå®é™…ä¿®æ”¹æ–‡ä»¶")
        # åœ¨è¯•è¿è¡Œæ¨¡å¼ä¸‹ï¼Œå¯ä»¥åªæ‰§è¡Œåˆ†ç±»å’Œåˆ†æï¼Œä¸æ‰§è¡Œå®é™…åˆå¹¶
    
    merger = DocumentMerger(args.docs_path)
    merger.run()

if __name__ == "__main__":
    main()